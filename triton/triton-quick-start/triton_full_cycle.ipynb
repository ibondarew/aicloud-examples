{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "828e30fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# QuickStart по работе с Triton \n",
    "\n",
    "Данный ноутбук демонстрирует полный цикл обучения, конвертации модели и запуск инференса Triton.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Что такое NVIDIA Triton?\n",
    "Triton Inference Server оптимизирует вывод ИИ, позволяя командам развертывать, запускать и масштабировать обученные модели ИИ из любой среды в любой инфраструктуре на основе графического процессора или процессора. Это дает исследователям искусственного интеллекта и специалистам по данным свободу выбора правильной платформы для своих проектов, не влияя на производственное развертывание\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547befb0",
   "metadata": {},
   "source": [
    "## Установка зависимостей "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d898560-14c3-48a2-8384-a1bfa391c5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "torch                     1.7.1+cu110\n",
      "torchaudio                0.7.2\n",
      "torchvision               0.8.2+cu110\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dcfadf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (4.21.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.7/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/conda/lib/python3.7/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/user/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/conda/lib/python3.7/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/conda/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/conda/lib/python3.7/site-packages (from transformers) (4.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/user/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: torch in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (1.7.1+cu110)\n",
      "Requirement already satisfied: typing-extensions in /home/user/conda/lib/python3.7/site-packages (from torch) (4.0.1)\n",
      "Requirement already satisfied: numpy in /home/user/conda/lib/python3.7/site-packages (from torch) (1.21.5)\n",
      "Requirement already satisfied: datasets in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/user/conda/lib/python3.7/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/user/conda/lib/python3.7/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (from datasets) (0.9.1)\n",
      "Requirement already satisfied: pandas in /home/user/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: responses<0.19 in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/user/conda/lib/python3.7/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/user/conda/lib/python3.7/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/conda/lib/python3.7/site-packages (from datasets) (4.10.1)\n",
      "Requirement already satisfied: multiprocess in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: packaging in /home/user/conda/lib/python3.7/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/conda/lib/python3.7/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: xxhash in /home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: aiohttp in /home/user/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/user/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.1)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/user/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/conda/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.7.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/user/conda/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/user/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.21.2\n",
    "!pip install torch #==1.7.1 \n",
    "!pip install datasets==2.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0290b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers 4.21.2\n",
      "torch 1.7.1+cu110\n",
      "datasets 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import transformers, torch, datasets\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"torch\", torch.__version__)\n",
    "print(\"datasets\", datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18a244",
   "metadata": {},
   "source": [
    "## Набор данных\n",
    "\n",
    "В этом примере используется датасет [emotion](https://huggingface.co/datasets/emotion). Этот датасет содержит набор сообщений из Twitter и размечен на 6 эмоций sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ca3e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-01 09:30:27.311060: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "Using custom data configuration default\n",
      "Reusing dataset emotion (/home/jovyan/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f099e925f8b42b5ad6211697b5e1b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b75a81",
   "metadata": {},
   "source": [
    "## Предобработка \n",
    "\n",
    "Этот этап необходим для предобработки текстовых сообщений (конвертации текста в вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeed7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e9895400dd461aa4fa67d37f735e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b76bd0b2fb34463b696b4d0ade1de82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba79e2d5c3c74808b0b1f3b2db8c77ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d1083",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7cbabf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da044865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a478c24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 02:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.113900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n",
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n",
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.1808741497039795, metrics={'train_runtime': 155.5923, 'train_samples_per_second': 514.164, 'train_steps_per_second': 32.135, 'total_flos': 973613755907712.0, 'train_loss': 0.1808741497039795, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ab005",
   "metadata": {},
   "source": [
    "# Инференс"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0712a6e4",
   "metadata": {},
   "source": [
    "Для удобства использования модели в инференсе, можно переименовать параметры с помощью словарей label2id и id2label. Это позволит при выводе результатов, видеть классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "535cb0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results/checkpoint-5000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./results/checkpoint-5000\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"sadness\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"love\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"fear\",\n",
      "    \"5\": \"surprise\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"fear\": 4,\n",
      "    \"joy\": 1,\n",
      "    \"love\": 2,\n",
      "    \"sadness\": 0,\n",
      "    \"surprise\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "label2id = {\n",
    "    \"sadness\": 0,\n",
    "    \"joy\": 1,\n",
    "    \"love\": 2,\n",
    "    \"anger\": 3,\n",
    "    \"fear\": 4,\n",
    "    \"surprise\": 5\n",
    "  }\n",
    "id2label = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "  }\n",
    "model_ckpt = \"./results/checkpoint-5000\"\n",
    "config = AutoConfig.from_pretrained(model_ckpt, label2id=label2id, id2label=id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231c070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./results/checkpoint-5000/added_tokens.json. We won't load it.\n",
      "loading file ./results/checkpoint-5000/vocab.txt\n",
      "loading file ./results/checkpoint-5000/tokenizer.json\n",
      "loading file None\n",
      "loading file ./results/checkpoint-5000/special_tokens_map.json\n",
      "loading file ./results/checkpoint-5000/tokenizer_config.json\n",
      "loading weights file ./results/checkpoint-5000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./results/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-5000\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./results/checkpoint-5000\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e04f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am incredibly happy to start using Triton on ML-Space from Cloud.ru\"\n",
    "\n",
    "tensor = tokenizer(text, padding=\"max_length\",  truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a9acf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example output SequenceClassifierOutput(loss=None, logits=tensor([[-1.9722,  7.3201, -2.4923, -1.7046, -2.8764, -1.7970]],\n",
      "       grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(\"Example output\", model(**tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5049380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'joy'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = model(**tensor).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "768e8e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "del config\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5f695c",
   "metadata": {},
   "source": [
    "## Подготовка модели к инференсу на Triton\n",
    "\n",
    "\n",
    "Для инференса модели на Triton необходимо PyTorch модель перевести в TorchScript. Для этой конвертации неоходимо показать модели пример входного и выходного вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df8c7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./results/checkpoint-5000/added_tokens.json. We won't load it.\n",
      "loading file ./results/checkpoint-5000/vocab.txt\n",
      "loading file ./results/checkpoint-5000/tokenizer.json\n",
      "loading file None\n",
      "loading file ./results/checkpoint-5000/special_tokens_map.json\n",
      "loading file ./results/checkpoint-5000/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-5000\")\n",
    "\n",
    "tensors = tokenizer(text, padding=\"max_length\",  truncation=True, return_tensors='pt', max_length=512)\n",
    "example_inputs = tensors['input_ids'], tensors['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4896789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PyTorch_to_TorchScript(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PyTorch_to_TorchScript, self).__init__()\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"./results/checkpoint-5000\")\n",
    "\n",
    "    def forward(self,data, attention_mask=None):\n",
    "        return self.model(data, attention_mask)[\"logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32ef9088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./results/checkpoint-5000/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./results/checkpoint-5000\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./results/checkpoint-5000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./results/checkpoint-5000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "pt_model = PyTorch_to_TorchScript().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6abcee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py:124: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  position_ids = self.position_ids[:, :seq_length]\n",
      "/home/jovyan/.imgenv-triton-ss-0/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py:215: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  mask, torch.tensor(torch.finfo(scores.dtype).min)\n"
     ]
    }
   ],
   "source": [
    "scripted_model = torch.jit.trace(pt_model, [tensors['input_ids'], tensors['attention_mask']], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfe49742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%self.1 : __torch__.___torch_mangle_295.PyTorch_to_TorchScript,\n",
       "      %input_ids : Long(1:512, 512:1, requires_grad=0, device=cpu),\n",
       "      %300 : Long(1:512, 512:1, requires_grad=0, device=cpu)):\n",
       "  %1399 : __torch__.transformers.models.distilbert.modeling_distilbert.___torch_mangle_294.DistilBertForSequenceClassification = prim::GetAttr[name=\"model\"](%self.1)\n",
       "  %1495 : Tensor = prim::CallMethod[name=\"forward\"](%1399, %input_ids, %300)\n",
       "  return (%1495)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripted_model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "174975a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9722,  7.3201, -2.4923, -1.7046, -2.8764, -1.7970]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs = scripted_model(tensors['input_ids'], tensors['attention_mask'])\n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5be8f6d5-6d81-4581-9cf4-c00e3e51e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f095cf7-6104-441a-8d7e-8c52851bde1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(list(outs.detach().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc4bdf75-fca5-43d5-8629-bd08bd5dbb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class': 'joy'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = outs.argmax().item()\n",
    "id2label = {\n",
    "    0: \"sadness\",\n",
    "    1: \"joy\",\n",
    "    2: \"love\",\n",
    "    3: \"anger\",\n",
    "    4: \"fear\",\n",
    "    5: \"surprise\"\n",
    "  }\n",
    "{\"class\": id2label[predicted_class_id]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4292f8",
   "metadata": {},
   "source": [
    "Перед сохранением модели необходимо создать каталог:\n",
    "\n",
    "```\n",
    "model_repository_path/\n",
    "|- <pytorch_model_name>/\n",
    "|  |- config.pbtxt\n",
    "|  |- 1/\n",
    "|     |- model.pt\n",
    "|\n",
    "```\n",
    "\n",
    "Где **pytorch_model_name** - название модели, **config.pbtxt** - конфигурация для Triton, **model.pt** - экспортированная модель. Структура каталогов будет выглядеть так:\n",
    "\n",
    "```\n",
    "triton_inf/\n",
    "|- / distil_bert_emotion\n",
    "|  |- config.pbtxt\n",
    "|  |- 1/\n",
    "|     |- model.pt\n",
    "|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e0bfb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: cannot create directory ‘Triton’: File exists\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: cannot create directory ‘Triton/Predictor’: File exists\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: cannot create directory ‘Triton/Predictor/distil_bert_emotion’: File exists\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "mkdir: cannot create directory ‘Triton/Predictor/distil_bert_emotion/1’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir Triton\n",
    "!mkdir Triton/Predictor\n",
    "!mkdir Triton/Predictor/distil_bert_emotion\n",
    "!mkdir Triton/Predictor/distil_bert_emotion/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5bf7c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted_model.save('./Triton/Predictor/distil_bert_emotion/1/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e5bd18",
   "metadata": {},
   "source": [
    "Теперь необходимо описать модель для Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d8870",
   "metadata": {},
   "source": [
    "Пример **config.pbtxt** \n",
    "```\n",
    "name: \"distil_bert_emotion\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  } ,\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1, 6]\n",
    "  }\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "Также можно заменить KIND_GPU на KIND_СPU, если используются ресурсы без GPU\n",
    "\n",
    "Где поле **name** - наименование модели,  **input** - описывает входной массив модели, **output** - описывает выходной массив. \n",
    "\n",
    "**input** указываются входные вектора. В этом примере у нас два входных вектора *input_ids* и *attention_mask* каждый имеет размерность `[1,512]` и тип данных `int32`. \n",
    "\n",
    "**output** указывает выходной вектор. В этом примере выходной вектор `[1,6]` и формат fp32\n",
    "\n",
    "Более подробно о написании **config.bptxt** можно ознакомиться в документации [Triton](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51d9f8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat > Triton/Predictor/distil_bert_emotion/config.pbtxt << EOF\n",
    "name: \"distil_bert_emotion\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "input [\n",
    " {\n",
    "    name: \"input__0\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  } ,\n",
    "{\n",
    "    name: \"input__1\"\n",
    "    data_type: TYPE_INT32\n",
    "    dims: [1, 512]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output__0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1, 6]\n",
    "  }\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "    }\n",
    "]\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1634657",
   "metadata": {},
   "source": [
    "## Transformer-скрипт"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f485c",
   "metadata": {},
   "source": [
    "Serving-скрипт отвечает за получение запроса, предобработку, отправку запроса в предиктор, постобработку предиктора.\n",
    "\n",
    "Для предобработки используется AutoTokenizer, ему необходимо указать откуда загрузить токенизатор.\n",
    "\n",
    "Для этого создадим директорию Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4687185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir Triton/Transformer\n",
    "!mkdir Triton/Transformer/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a594fb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cp results/checkpoint-5000/tokenizer.json Triton/Transformer/tokenizer\n",
    "!cp results/checkpoint-5000/tokenizer_config.json Triton/Transformer/tokenizer\n",
    "!cp results/checkpoint-5000/vocab.txt Triton/Transformer/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b63a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 936\n",
      "-rw-r--r-- 1 jovyan jovyan 711494 Sep  1 09:33 tokenizer.json\n",
      "-rw-r--r-- 1 jovyan jovyan    360 Sep  1 09:33 tokenizer_config.json\n",
      "-rw-r--r-- 1 jovyan jovyan 231508 Sep  1 09:33 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -l Triton/Transformer/tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ea3800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer/kf_serving.py\n",
    "\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import kfserving\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class BertTransformer(kfserving.KFModel):\n",
    "    def __init__(self, name: str, predictor_host: str):\n",
    "        super().__init__(name)\n",
    "        self.predictor_host = predictor_host\n",
    "        # токенайзер с сохранеными файлами\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('./tokenizer') \n",
    "        # наименование модели (из configb.pbtxt)\n",
    "        self.model_name = \"distil_bert_emotion\" \n",
    "        self.triton_client = None\n",
    "        \n",
    "        # Словарь с сопоставлением: \"порядок в векторе выхода сети\" -> \"эмоция\"\n",
    "        self.id2label = {\n",
    "            0: \"sadness\",\n",
    "            1: \"joy\",\n",
    "            2: \"love\",\n",
    "            3: \"anger\",\n",
    "            4: \"fear\",\n",
    "            5: \"surprise\"\n",
    "          }\n",
    "\n",
    "    def preprocess(self, inputs: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "            Препроцесинг входных данных \n",
    "        \"\"\"\n",
    "         # токенезируем входной запрос\n",
    "        tensors = self.tokenizer(inputs[\"instances\"][0], padding=\"max_length\",  truncation=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "        return {\"input__0\":tensors['input_ids'], \"input__1\":tensors['attention_mask']}\n",
    "\n",
    "    def predict(self, features: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "            Предикт     \n",
    "        \"\"\"\n",
    "        if not self.triton_client:\n",
    "            self.triton_client = httpclient.InferenceServerClient(\n",
    "                url=self.predictor_host, verbose=True)\n",
    "\n",
    "        input__0 = np.array(features['input__0'], dtype=np.int32) # конвертируем вектор  в int32\n",
    "        input__1 = np.array(features['input__1'], dtype=np.int32) # конвертируем вектор  в int32\n",
    "\n",
    "        input__0 = input__0.reshape(1, 512) # преобразуем в [1,512]\n",
    "        input__1 = input__1.reshape(1, 512)  # преобразуем в [1,512]\n",
    "\n",
    "        # Формируем запрос в тритон\n",
    "        inputs = [httpclient.InferInput('input__0', [1, 512], \"INT32\"), \n",
    "                  httpclient.InferInput('input__1', [1, 512], \"INT32\")]  \n",
    "        # Заполняем запрос данными из numpy массива\n",
    "        inputs[0].set_data_from_numpy(input__0) \n",
    "        inputs[1].set_data_from_numpy(input__1)\n",
    "\n",
    "        \n",
    "        # Указываем ожидаемый выходной результат сети\n",
    "        outputs = [httpclient.InferRequestedOutput('output__0', binary_data=False),] \n",
    "        result = self.triton_client.infer(self.model_name, inputs, outputs=outputs)\n",
    "        return result.get_response()\n",
    "\n",
    "    def postprocess(self, result: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "            Обработка результата сети\n",
    "        \"\"\"\n",
    "        logging.info(result)\n",
    "        prediction = result['outputs'][0]['data']\n",
    "        predicted_class_id = np.argmax(prediction)\n",
    "\n",
    "        return {\"predictions\": self.id2label[predicted_class_id]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--http-port\", default=8080)\n",
    "    parser.add_argument(\"--predictor-host\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    x = re.compile('(kfserving-\\d+)').search(os.environ.get('HOSTNAME'))\n",
    "    name = \"kfserving-default\"\n",
    "    if x:\n",
    "        name = x[0]\n",
    "    model = BertTransformer(name, predictor_host=args.predictor_host)\n",
    "    kfserving.KFServer(workers=1, http_port=args.http_port).start([model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba6677",
   "metadata": {},
   "source": [
    "Сформированный скрипт для сервинга модели необходимо сохранить по пути `Triton/Transformer/kf_serving.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ad90f",
   "metadata": {},
   "source": [
    "Для работы kf_serving.py скрипта необходимо добавить в установку используемые в нем зависимости. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c824251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    " \n",
    "cat >> Triton/Transformer/requirements.txt << EOF\n",
    "tritonclient [all]\n",
    "transformers\n",
    "torch\n",
    "numpy\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb1bd2",
   "metadata": {},
   "source": [
    "Итоговая структура директории:\n",
    "```\n",
    " |-Triton\n",
    " | |-Transformer\n",
    " | | |-tokenizer\n",
    " | | | |-tokenizer.json\n",
    " | | | |-tokenizer_config.json\n",
    " | | | |-vocab.txt\n",
    " | | |-requirements.txt\n",
    " | | |-kf_serving.py\n",
    " | |-Predictor\n",
    " | | |-distil_bert_emotion\n",
    " | | | |-1\n",
    " | | | | |-model.pt\n",
    " | | | |-config.pbtxt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525423c3",
   "metadata": {},
   "source": [
    "## Создание образа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c81451",
   "metadata": {},
   "source": [
    "Для сборки образа необходимо созданные папки **Transformer** и **Predictor** загрузить в бакет S3. Если бакет создан, то нужно перейти в раздел получения credentials. Для создания бакета S3 Data Catalog -> Обзор Хранилища -> Создать Бакет.\n",
    "\n",
    "\n",
    "<img src=\"img/data_storage.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "![data storage](img/storage.png)\n",
    "\n",
    "\n",
    "После создания бакета необходимо получить его credentials для подключения с помощью сторонних утилит и последующей загрузки файлов. \n",
    "\n",
    "\n",
    "<img src=\"img/get_cred.png\" alt=\"drawing\" width=\"500\"/>\n",
    "<img src=\"img/view_cred.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "\n",
    "После того как получили credentials необходимо скопировать: \n",
    "- S3 endpoint\n",
    "- S3 имя бакета\n",
    "- S3 access key ID\n",
    "- S3 security key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5525650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "S3_ACCESS_KEY_ID = \"USER_S3_ACCESS_KEY_ID\"\n",
    "S3_SECRET_ACCESS_KEY_ID = \"S3_SECRET_ACCESS_KEY_ID\"\n",
    "BUCKET_NAME = \"BUCKET_NAME\"\n",
    "ENDPOINT_URL = \"ENDPOINT_URL\"\n",
    "\n",
    "def upload_files(bucket, path):\n",
    "    session = boto3.session.Session()\n",
    " \n",
    "    s3_client = session.client(\n",
    "        service_name='s3',\n",
    "        aws_access_key_id=S3_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=S3_SECRET_ACCESS_KEY_ID,\n",
    "        endpoint_url=ENDPOINT_URL\n",
    "    )\n",
    " \n",
    "    for subdir, dirs, files in tqdm(os.walk(path)):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            with open(full_path, 'rb') as data:\n",
    "                s3_client.put_object(Bucket = bucket, Key=full_path[len(path)+1:], Body=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2127c56f",
   "metadata": {},
   "source": [
    "Загрузим каталоги из Triton в S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51216378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:06,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "upload_files(BUCKET_NAME, './Triton')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3d5ae",
   "metadata": {},
   "source": [
    "После загрузки можем приступить к сборке образа. Для сборки образа зайти в Deployment->Образы и нажмите \"Создать образ\"\n",
    "\n",
    "\n",
    "<img src=\"img/image.png\" alt=\"drawing\" width=\"900\"/>\n",
    "\n",
    "\n",
    "Первым образом, соберем \"Трансформер\".\n",
    "\n",
    "1. Тип образа  - Triton Server\n",
    "2. Тип контейнера - Трансформер\n",
    "3. Базовый образ - cr.msk.sbercloud.ru/aicloud-base-images/triton22.04-py3:0.0.32 \n",
    "4. Хранилище - тот S3 бакет в который загружали ранее \n",
    "5. Конфигурация\n",
    "    - Папка с моделью -  Transformer \n",
    "    - Файл Serving-script - kf_serving.py\n",
    "    - Файл Requirements - requirements.txt\n",
    "    \n",
    "<img src=\"img/image_build_transformer.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Вторым образом, соберем \"Предиктор\".\n",
    "\n",
    "1. Тип образа  - Triton Server\n",
    "2. Тип контейнера - Предиктор\n",
    "3. Базовый образ - cr.msk.sbercloud.ru/aicloud-base-images/triton22.04-py3:0.0.32 \n",
    "4. Хранилище - тот S3 бакет в который загружали ранее \n",
    "5. Конфигурация\n",
    "    - Папка с файлами конфигурации - папка с моделью. Пример - ИМЯ_БАКЕТА/Predictor\n",
    "\n",
    "\n",
    "<img src=\"img/image_build_predictor.png\" alt=\"\" width=\"900\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bd139",
   "metadata": {},
   "source": [
    "## Деплой\n",
    "\n",
    "\n",
    "Для деплоя модели зайдите в Deployment -> Деплои\n",
    "\n",
    "\n",
    "<img src=\"img/deploi.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Нажмите кнопку \"Создать деплой\". Укажите следующие настройки. \n",
    "\n",
    "1. Наименование - Название сборки (можно оставить пустым)\n",
    "2. Тип деплоя - Раздельный\n",
    "3. Ресурсы - указываем регион и тип конфигурации \n",
    "4. Указываем долю ресурсов от общей конфигурации для контейнера Transformer\n",
    "5. Выберите Docker-образ - указываете собранные Docker собранные ранее \n",
    "\n",
    "\n",
    "<img src=\"img/create_deploi.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "\n",
    "\n",
    "После создания, появится карточка с созданным деплоем, со статусом **\"В очереди\"**. То есть данный деплой находиться на стадии ожидания выбранных ресурсов и как только ресурсы станут доступны, деплой передает в статус **\"Выполняется\"**\n",
    "\n",
    "Обратите внимание, что если минимальное количество Pods будет установлено в \"0\", то горячий деплой не будет запущен сразу. В таком случае при первом запросе, вы получите дополнительную задержку на поднятии деплоя. \n",
    "\n",
    "\n",
    "Открыв карточку с запущеным деплоем можно посмотреть и изменить текущую конфигурацию.\n",
    "\n",
    "<img src=\"img/image_triron.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Так же можно отправить тестовый запрос из вкладки \"Тест API\" и скопировать его в виде cURL \n",
    "\n",
    "<img src=\"img/image_example_requests.png\" alt=\"\" width=\"900\"/>\n",
    "\n",
    "Во вкладке \"Логи\" можно посмотреть текущее состояние деплоя Triton \n",
    "\n",
    "<img src=\"img/example_logs.png\" alt=\"\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c374dc4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e740bb003c59f442552ef7e876805227e11287db79117ba91a0c5bdb1045b2fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
